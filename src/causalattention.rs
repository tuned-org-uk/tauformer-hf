//! NanoGPT with numerically stable attention and logits
//!
//! Key stability measures:
//! - Stable attention softmax (max-subtraction + large-negative mask)
//! - Robust RoPE broadcasting (align [1,T,1,D/2] → [B,H,T,D/2])
//! - LayerNorm (stable) for QK-norm and block prenorm
//! - Kaiming init with reduced gain, QKV projection clamping
//! - Logits clamped before/after softcap tanh

use burn::{
    module::Module,
    nn::{Embedding, EmbeddingConfig, LayerNorm, LayerNormConfig, Linear, LinearConfig},
    tensor::{Bool, Int, Tensor, activation, backend::Backend},
};
use log::{debug, info, trace};
use std::path::Path;

use crate::config::NanoChatConfig;
use crate::engine::GptCache;
use crate::rope::{apply_rotary_emb, apply_rotary_emb_step, precompute_rotary_embeddings};

// ─────────────────────────────────────────────────────────────────────────────
// RMS norm
// ─────────────────────────────────────────────────────────────────────────────

// Functional RMSNorm (no learnable params), over last dimension
pub fn rms_norm<B: Backend, const D: usize>(x: Tensor<B, D>, eps: f32) -> Tensor<B, D> {
    let dims = x.dims();
    let last = dims.len() - 1;

    // mean of squares over last dim
    let ms = x.clone().powf_scalar(2.0).mean_dim(last);
    let rms = (ms + eps).sqrt();

    // broadcast back to input shape
    let mut bshape = dims.clone();
    bshape[last] = 1;
    let rms_b = rms.reshape(bshape).expand(dims);

    x / rms_b
}

// ─────────────────────────────────────────────────────────────────────────────
// Attention (LayerNorm-based QK-norm for stability)
// ─────────────────────────────────────────────────────────────────────────────

#[derive(Module, Debug)]
pub struct CausalSelfAttention<B: Backend> {
    layer_idx: usize,
    n_head: usize,
    n_kv_head: usize,
    head_dim: usize,
    c_q: Linear<B>,
    c_k: Linear<B>,
    c_v: Linear<B>,
    c_proj: Linear<B>,
    qk_norm_q: LayerNorm<B>,
    qk_norm_k: LayerNorm<B>,
}

impl<B: Backend> CausalSelfAttention<B> {
    pub fn new(config: &NanoChatConfig, layer_idx: usize, device: &B::Device) -> Self {
        let n_head = config.n_head;
        let n_kv_head = config.n_kv_head;
        let n_embd = config.n_embd;
        let head_dim = n_embd / n_head;

        assert_eq!(n_embd % n_head, 0, "n_embd must be divisible by n_head");
        assert!(
            n_kv_head <= n_head && n_head % n_kv_head == 0,
            "Invalid MQA config"
        );

        debug!(
            "Layer {}: Attn n_head={}, n_kv_head={}, head_dim={}",
            layer_idx, n_head, n_kv_head, head_dim
        );

        let init = burn::nn::Initializer::KaimingUniform {
            gain: 0.5,
            fan_out_only: false,
        };

        Self {
            layer_idx,
            n_head,
            n_kv_head,
            head_dim,
            c_q: LinearConfig::new(n_embd, n_head * head_dim)
                .with_bias(false)
                .with_initializer(init.clone())
                .init(device),
            c_k: LinearConfig::new(n_embd, n_kv_head * head_dim)
                .with_bias(false)
                .with_initializer(init.clone())
                .init(device),
            c_v: LinearConfig::new(n_embd, n_kv_head * head_dim)
                .with_bias(false)
                .with_initializer(init.clone())
                .init(device),
            c_proj: LinearConfig::new(n_embd, n_embd)
                .with_bias(false)
                .with_initializer(init)
                .init(device),
            qk_norm_q: LayerNormConfig::new(head_dim).init(device),
            qk_norm_k: LayerNormConfig::new(head_dim).init(device),
        }
    }

    pub fn forward(
        &self,
        x: Tensor<B, 3>, // [B, T, C]
        cos_sin: (&Tensor<B, 4>, &Tensor<B, 4>),
    ) -> Tensor<B, 3> {
        let [b, t, c] = x.dims();
        debug!(
            "Layer {} attn forward: input [B={}, T={}, C={}]",
            self.layer_idx, b, t, c
        );

        // Projections with clipping
        let q = self.c_q.forward(x.clone()).clamp(-5.0, 5.0).reshape([
            b,
            t,
            self.n_head,
            self.head_dim,
        ]);
        let k = self.c_k.forward(x.clone()).clamp(-5.0, 5.0).reshape([
            b,
            t,
            self.n_kv_head,
            self.head_dim,
        ]);
        let v = self
            .c_v
            .forward(x)
            .clamp(-5.0, 5.0)
            .reshape([b, t, self.n_kv_head, self.head_dim]);

        debug!(
            "Layer {} QKV shapes: Q {:?}, K {:?}, V {:?}",
            self.layer_idx,
            q.dims(),
            k.dims(),
            v.dims()
        );

        // [B, H, T, D]
        let q = q.swap_dims(1, 2);
        let k = k.swap_dims(1, 2);
        let v = v.swap_dims(1, 2);

        // Apply RoPE (now slicing happens INSIDE apply_rotary_emb)
        let (cos, sin) = cos_sin;
        let q = apply_rotary_emb(q, cos, sin); // cos/sin are full cache
        let k = apply_rotary_emb(k, cos, sin);

        // DEBUG: Extract position 2 from both Q and K before normalization
        if t >= 3 {
            let q_pos2 = q.clone().slice([0..1, 0..1, 2..3, 0..4]).reshape([4]);
            let k_pos2 = k.clone().slice([0..1, 0..1, 2..3, 0..4]).reshape([4]);
            let q_vec: Vec<f32> = q_pos2.to_data().to_vec().unwrap();
            let k_vec: Vec<f32> = k_pos2.to_data().to_vec().unwrap();
            debug!("Q[pos=2, head=0]: {:?}", q_vec);
            debug!("K[pos=2, head=0]: {:?}", k_vec);
        }

        debug!(
            "Layer {} after RoPE: Q {:?}, K {:?}",
            self.layer_idx,
            q.dims(),
            k.dims()
        );

        // QK-norm via LayerNorm over D
        let q = rms_norm(q, 1e-6);
        let k = rms_norm(k, 1e-6);

        // DEBUG: Check Q/K after normalization
        if t >= 3 {
            let q_pos2_norm = q.clone().slice([0..1, 0..1, 2..3, 0..4]).reshape([4]);
            let k_pos2_norm = k.clone().slice([0..1, 0..1, 2..3, 0..4]).reshape([4]);
            let q_vec_norm: Vec<f32> = q_pos2_norm.to_data().to_vec().unwrap();
            let k_vec_norm: Vec<f32> = k_pos2_norm.to_data().to_vec().unwrap();
            debug!("Q_NORM[pos=2, head=0]: {:?}", q_vec_norm);
            debug!("K_NORM[pos=2, head=0]: {:?}", k_vec_norm);
        }

        debug!("Layer {} after RMS-norm", self.layer_idx);

        let q = self.norm_heads(q, &self.qk_norm_q);
        let k = self.norm_heads(k, &self.qk_norm_k);

        debug!("Layer {} after norm_heads", self.layer_idx);

        // Attention
        let y = self.scaled_dot_product_attention(q, k, v, t, t);
        debug!("Layer {} attention output: {:?}", self.layer_idx, y.dims());

        // [B, T, C]
        let y = y.swap_dims(1, 2).reshape([b, t, c]);
        self.c_proj.forward(y)
    }

    fn norm_heads(&self, x: Tensor<B, 4>, ln: &LayerNorm<B>) -> Tensor<B, 4> {
        let [b, h, t, d] = x.dims();
        let xflat = x.reshape([b * h * t, d]);
        let out = ln.forward(xflat);
        out.reshape([b, h, t, d])
    }

    fn scaled_dot_product_attention(
        &self,
        q: Tensor<B, 4>, // [B, H, Tq, D]
        k: Tensor<B, 4>, // [B, H, Tk, D]
        v: Tensor<B, 4>, // [B, H, Tk, D]
        t_q: usize,
        t_k: usize,
    ) -> Tensor<B, 4> {
        let [b, _hq, _tq, d] = q.dims();
        let h_kv = k.dims()[1];

        debug!(
            "Attn(L{}): q {:?}, k {:?}, v {:?}, Tq={}, Tk={}, D={}, H_kv={}",
            self.layer_idx,
            q.dims(),
            k.dims(),
            v.dims(),
            t_q,
            t_k,
            d,
            h_kv
        );

        // MQA repeat if needed
        let (k, v) = if self.n_head != self.n_kv_head {
            let repeat = self.n_head / self.n_kv_head;
            trace!(
                "Attn(L{}): MQA expand KV heads: H_kv={} -> H_q={} (repeat x{})",
                self.layer_idx, self.n_kv_head, self.n_head, repeat
            );
            let k5: Tensor<B, 5> = k.unsqueeze_dim::<5>(2);
            let k = k5
                .expand([b, h_kv, repeat, t_k, d])
                .reshape([b, self.n_head, t_k, d]);
            let v5: Tensor<B, 5> = v.unsqueeze_dim::<5>(2);
            let v = v5
                .expand([b, h_kv, repeat, t_k, d])
                .reshape([b, self.n_head, t_k, d]);
            (k, v)
        } else {
            (k, v)
        };

        trace!(
            "Attn(L{}): after MQA (if any): k {:?}, v {:?}",
            self.layer_idx,
            k.dims(),
            v.dims()
        );

        // Compute raw attention scores
        let scale = (d as f32).sqrt();
        let mut att = q.matmul(k.swap_dims(2, 3)) / scale; // [B, H, Tq, Tk]
        trace!("Attn(L{}): raw att {:?}", self.layer_idx, att.dims());

        // 1) Apply causal mask FIRST with large negative
        // Align queries to the end of the key cache:
        // - forward: t_q == t_k => diag = 0 (standard causal mask)
        // - decode:  t_q == 1,  t_k == T => diag = T-1 (allow all past keys)
        let diag = (t_k as i64) - (t_q as i64);
        let mask2: Tensor<B, 2, Bool> = Tensor::tril_mask([t_q, t_k], diag, &att.device());

        // Burn's tril_mask returns FALSE for lower-tri (allowed), TRUE for upper-tri (blocked)
        // mask_fill replaces where mask is TRUE, so we want upper-tri to be TRUE
        // Therefore: use mask2 DIRECTLY (no bool_not)
        let mask4 = mask2
            .unsqueeze_dims::<4>(&[0, 1])
            .expand([b, self.n_head, t_q, t_k]);

        att = att.mask_fill(mask4, -1.0e9); // Fill upper-tri (where mask=true) with -1e9
        trace!("Attn(L{}): causal mask applied", self.layer_idx);

        // 2) Subtract per-row max along keys axis (dimension 3)
        let att_max = att.clone().max_dim(3).squeeze::<3>(3); // [B, H, Tq]
        att = att - att_max.unsqueeze_dim::<4>(3); // [B, H, Tq, Tk]
        trace!(
            "Attn(L{}): stabilized by row max subtraction",
            self.layer_idx
        );

        // 3) Softmax over keys axis
        let att = activation::softmax(att, 3);

        // DEBUG: Check attention weights for position 2
        if t_q >= 3 {
            let att_pos2 = att.clone().slice([0..1, 0..1, 2..3, 0..t_k]).reshape([t_k]);
            let att_vec: Vec<f32> = att_pos2.to_data().to_vec().unwrap();
            debug!("ATT_WEIGHTS[pos=2, head=0]: {:?}", att_vec);
        }

        trace!("Attn(L{}): softmax done on axis=3", self.layer_idx);

        // 4) Weighted sum
        let out = att.matmul(v); // [B, H, Tq, D]
        debug!("Attn(L{}): output shape {:?}", self.layer_idx, out.dims());
        out
    }

    // Decode-time forward: Tq = 1, appends K/V to cache for this layer and attends to full past.
    pub fn forward_decode(
        &self,
        x_step: Tensor<B, 3>,                                   // [B, 1, C]
        cos_sin_step: (&Tensor<B, 4>, &Tensor<B, 4>),           // [1,1,1,D/2]
        cache_layer: &mut Option<(Tensor<B, 4>, Tensor<B, 4>)>, // K/V store for this layer
    ) -> Tensor<B, 3> {
        let [b, tq, c] = x_step.dims();
        debug_assert_eq!(tq, 1, "forward_decode expects T=1 input");
        debug!(
            "Layer {} decode: x_step shape [B={}, T={}, C={}]",
            self.layer_idx, b, tq, c
        );

        // Project Q,K,V then reshape
        let q = self
            .c_q
            .forward(x_step.clone())
            .clamp(-5.0, 5.0)
            .reshape([b, 1, self.n_head, self.head_dim])
            .swap_dims(1, 2); // [B, Hq, 1, D]

        let k_new = self
            .c_k
            .forward(x_step.clone())
            .clamp(-5.0, 5.0)
            .reshape([b, 1, self.n_kv_head, self.head_dim])
            .swap_dims(1, 2); // [B, Hkv, 1, D]

        let v_new = self
            .c_v
            .forward(x_step)
            .clamp(-5.0, 5.0)
            .reshape([b, 1, self.n_kv_head, self.head_dim])
            .swap_dims(1, 2); // [B, Hkv, 1, D]

        debug!(
            "Layer {} decode: Q/K/V step shapes -> Q {:?}, K_new {:?}, V_new {:?}",
            self.layer_idx,
            q.dims(),
            k_new.dims(),
            v_new.dims()
        );

        // Apply RoPE to Q and to K_new
        let (cos_step, sin_step) = cos_sin_step;
        debug!(
            "Layer {} decode: RoPE step cos/sin shapes: {:?} / {:?}",
            self.layer_idx,
            cos_step.dims(),
            sin_step.dims()
        );

        // Apply RoPE to Q and K_new at this absolute position
        let q = apply_rotary_emb_step(q, cos_step, sin_step);
        let k_new = apply_rotary_emb_step(k_new, cos_step, sin_step);

        // Then normalize (same as forward)
        let q = rms_norm(q, 1e-6);
        let k_new = rms_norm(k_new, 1e-6);

        let q = self.norm_heads(q, &self.qk_norm_q);
        let k_new = self.norm_heads(k_new, &self.qk_norm_k);
        debug!("Layer {} decode: after QK-norm", self.layer_idx);

        // Append new K,V into cache (time concat on dim=2)
        let (k_full, v_full): (Tensor<B, 4>, Tensor<B, 4>) = match cache_layer.take() {
            Some((k_all, v_all)) => {
                let tk_prev = k_all.dims()[2];
                let k_cat = Tensor::cat(vec![k_all, k_new.clone()], 2);
                let v_cat = Tensor::cat(vec![v_all, v_new.clone()], 2);
                debug!(
                    "Layer {} decode: appended to cache (prev T={}, new T={})",
                    self.layer_idx,
                    tk_prev,
                    tk_prev + 1
                );
                (k_cat, v_cat)
            }
            None => {
                debug!(
                    "Layer {} decode: initializing cache (T becomes 1)",
                    self.layer_idx
                );
                (k_new.clone(), v_new.clone())
            }
        };

        // Store back into the cache layer
        *cache_layer = Some((k_full.clone(), v_full.clone()));

        // Use full K,V from cache for attention
        let tk = k_full.dims()[2];
        debug!(
            "Layer {} decode: attention with Tq=1, Tk={}, heads_q={}, heads_kv={}",
            self.layer_idx, tk, self.n_head, self.n_kv_head
        );
        let y = self.scaled_dot_product_attention(q, k_full, v_full, 1, tk);

        // Merge heads to [B,1,C] then project
        let y = y.swap_dims(1, 2).reshape([b, 1, c]);
        let out = self.c_proj.forward(y);
        debug!(
            "Layer {} decode: output step shape {:?}",
            self.layer_idx,
            out.dims()
        );
        out
    }
}

// ─────────────────────────────────────────────────────────────────────────────
// MLP (ReLU²) and Block (pre-norm)
// ─────────────────────────────────────────────────────────────────────────────

#[derive(Module, Debug)]
pub struct Mlp<B: Backend> {
    c_fc: Linear<B>,
    c_proj: Linear<B>,
}

impl<B: Backend> Mlp<B> {
    pub fn new(cfg: &NanoChatConfig, device: &B::Device) -> Self {
        let n = cfg.n_embd;
        debug!("MLP init: n_embd={}, hidden=4*n_embd={}", n, 4 * n);

        let init = burn::nn::Initializer::KaimingUniform {
            gain: 0.5,
            fan_out_only: false,
        };

        Self {
            c_fc: LinearConfig::new(n, 4 * n)
                .with_bias(false)
                .with_initializer(init.clone())
                .init(device),
            c_proj: LinearConfig::new(4 * n, n)
                .with_bias(false)
                .with_initializer(init)
                .init(device),
        }
    }

    pub fn forward(&self, x: Tensor<B, 3>) -> Tensor<B, 3> {
        debug!("MLP forward: input shape {:?}", x.dims());
        let x = self.c_fc.forward(x);
        let x = activation::relu(x).powf_scalar(2.0);
        self.c_proj.forward(x)
    }
}

#[derive(Module, Debug)]
pub struct Block<B: Backend> {
    layer_idx: usize,
    ln1: LayerNorm<B>,
    attn: CausalSelfAttention<B>,
    ln2: LayerNorm<B>,
    mlp: Mlp<B>,
}

impl<B: Backend> Block<B> {
    pub fn new(cfg: &NanoChatConfig, layer_idx: usize, device: &B::Device) -> Self {
        info!("Initializing Block {}", layer_idx);
        Self {
            layer_idx,
            ln1: LayerNormConfig::new(cfg.n_embd).init(device),
            attn: CausalSelfAttention::new(cfg, layer_idx, device),
            ln2: LayerNormConfig::new(cfg.n_embd).init(device),
            mlp: Mlp::new(cfg, device),
        }
    }

    pub fn forward(
        &self,
        x: Tensor<B, 3>,
        cos_sin: (&Tensor<B, 4>, &Tensor<B, 4>), // references now
    ) -> Tensor<B, 3> {
        let x = x.clone() + self.attn.forward(rms_norm(x.clone(), 1e-6), cos_sin);
        x.clone() + self.mlp.forward(rms_norm(x, 1e-6))
    }

    pub fn forward_decode(
        &self,
        x_step: Tensor<B, 3>,                         // [B,1,C]
        cos_sin_step: (&Tensor<B, 4>, &Tensor<B, 4>), // [1,1,1,D/2]
        cache_layer: &mut Option<(Tensor<B, 4>, Tensor<B, 4>)>,
    ) -> Tensor<B, 3> {
        let x = x_step.clone()
            + self
                .attn
                .forward_decode(rms_norm(x_step.clone(), 1e-6), cos_sin_step, cache_layer);
        x.clone() + self.mlp.forward(rms_norm(x, 1e-6))
    }
}

// ─────────────────────────────────────────────────────────────────────────────
// GPT
// ─────────────────────────────────────────────────────────────────────────────

#[derive(Module, Debug)]
pub struct GptModel<B: Backend> {
    wte: Embedding<B>,
    blocks: Vec<Block<B>>,
    ln_f: LayerNorm<B>,
    lm_head: Linear<B>,
    cos: Tensor<B, 4>,
    sin: Tensor<B, 4>,
    n_embd: usize,
}

impl<B: Backend> GptModel<B> {
    pub fn new(cfg: &NanoChatConfig, device: &B::Device) -> Self {
        info!("═══════════════════════════════════════");
        info!("Initializing GptModel (stable version)");
        info!("  vocab_size: {}", cfg.vocab_size);
        info!("  n_layer: {}", cfg.n_layer);
        info!("  n_head: {}", cfg.n_head);
        info!("  n_kv_head: {}", cfg.n_kv_head);
        info!("  n_embd: {}", cfg.n_embd);
        info!("  sequence_len: {}", cfg.sequence_len);
        info!("═══════════════════════════════════════");

        let head_dim = cfg.n_embd / cfg.n_head;
        let (cos, sin) =
            precompute_rotary_embeddings(cfg.sequence_len * 10, head_dim, 10000.0, device);

        info!("Creating embedding layer");
        let wte = EmbeddingConfig::new(cfg.vocab_size, cfg.n_embd).init(device);

        info!("Creating {} transformer blocks", cfg.n_layer);
        let blocks = (0..cfg.n_layer)
            .map(|i| Block::new(cfg, i, device))
            .collect();

        info!("Creating final LayerNorm and lm_head");
        let ln_f = LayerNormConfig::new(cfg.n_embd).init(device);
        let lm_head = LinearConfig::new(cfg.n_embd, cfg.vocab_size)
            .with_bias(false)
            .with_initializer(burn::nn::Initializer::KaimingUniform {
                gain: 0.5,
                fan_out_only: false,
            })
            .init(device);

        info!("Model initialization complete");
        Self {
            wte,
            blocks,
            ln_f,
            lm_head,
            cos,
            sin,
            n_embd: cfg.n_embd,
        }
    }

    pub fn forward(&self, idx: Tensor<B, 2, Int>, use_softcap: bool) -> Tensor<B, 3> {
        let [b, t] = idx.dims();
        assert!(t > 0, "Sequence length must be > 0");
        debug!("GptModel.forward: input [B={}, T={}]", b, t);

        // DON'T slice - pass full cache
        debug!(
            "RoPE cache (full): cos {:?}, sin {:?}",
            self.cos.dims(),
            self.sin.dims()
        );

        // Embed
        let mut x = self.wte.forward(idx);
        debug!("After embedding: shape {:?}", x.dims());

        // Blocks (pass full cache as references)
        for (i, block) in self.blocks.iter().enumerate() {
            x = block.forward(x, (&self.cos, &self.sin));
            debug!("After block {}: shape {:?}", i, x.dims());
        }

        // Final norm
        x = rms_norm(x, 1e-6);
        debug!("After final RMS norm: shape {:?}", x.dims());

        // Head → logits
        let mut logits = self.lm_head.forward(x);
        debug!("After lm_head (before clamp): shape {:?}", logits.dims());

        // Safety clamp before softcap
        logits = logits.clamp(-50.0, 50.0);

        // Softcap
        if use_softcap {
            let softcap = 15.0;
            debug!("Applying softcap={}", softcap);
            logits = logits
                .clone()
                .div_scalar(softcap)
                .tanh()
                .mul_scalar(softcap);
        }

        // Final clamp
        logits = logits.clamp(-50.0, 50.0);
        debug!("Final logits shape: {:?}", logits.dims());
        logits
    }

    pub fn generate(&self, mut idx: Tensor<B, 2, Int>, max_new_tokens: usize) -> Tensor<B, 2, Int> {
        let [_b, t0] = idx.dims();
        info!("Generation: initial_len={}, max_new={}", t0, max_new_tokens);

        for step in 0..max_new_tokens {
            let logits = self.forward(idx.clone(), true);
            let [b, t, v] = logits.dims();

            if step % 5 == 0 {
                debug!("Generation step {}: seq_len={}", step, t);
            }

            let last = logits.slice([0..b, (t - 1)..t, 0..v]).reshape([b, v]);
            let next = last.argmax(1).reshape([b, 1]);
            idx = Tensor::cat(vec![idx, next], 1);
        }

        info!("Generation complete: final_len={}", idx.dims()[1]);
        idx
    }

    // Test-only: no softcap
    #[cfg(test)]
    pub fn forward_no_softcap(&self, idx: Tensor<B, 2, Int>) -> Tensor<B, 3> {
        debug!("GptModel -> forward NO soft cap");
        self.forward(idx, false)
    }

    pub fn check_logits_health(logits: &Tensor<B, 3>) -> bool {
        let data = logits.clone().to_data();
        let vec: Vec<f32> = data.to_vec().unwrap();
        let is_healthy = vec.iter().all(|&x| x.is_finite());
        if !is_healthy {
            debug!("⚠️  Logits contain NaN or Inf!");
        }
        is_healthy
    }

    // Number of blocks (for KVCache sizing)
    pub fn num_layers(&self) -> usize {
        self.blocks.len()
    }

    // Decode one step using KV cache: last_ids [B,1] -> logits [B,1,V]
    pub fn forward_decode(
        &self,
        last_ids: Tensor<B, 2, Int>, // [B,1]
        cache: &mut crate::engine::KVCache<B>,
        use_softcap: bool,
    ) -> Tensor<B, 3> {
        let [b, tq] = last_ids.dims();
        debug_assert_eq!(tq, 1);

        // Determine current time position from cache
        let t_pos = cache.position();

        // Slice RoPE for current position: [1,1,1,D/2]
        let head_dim = self.cos.dims()[3];
        let cos_step = self
            .cos
            .clone()
            .slice([0..1, t_pos..(t_pos + 1), 0..1, 0..head_dim]);
        let sin_step = self
            .sin
            .clone()
            .slice([0..1, t_pos..(t_pos + 1), 0..1, 0..head_dim]);

        // Embed last token
        let mut x = self.wte.forward(last_ids).reshape([b, 1, self.n_embd]);

        // One-step through blocks with KV cache
        for (i, block) in self.blocks.iter().enumerate() {
            let layer_cache = &mut cache.store[i];
            x = block.forward_decode(x, (&cos_step, &sin_step), layer_cache);
        }

        // Final LayerNorm -> RMSNorm
        x = rms_norm(x, 1e-6);

        let mut logits = self.lm_head.forward(x);

        // Stability clamps and optional softcap
        logits = logits.clamp(-50.0, 50.0);
        if use_softcap {
            let softcap = 15.0;
            logits = logits
                .clone()
                .div_scalar(softcap)
                .tanh()
                .mul_scalar(softcap);
        }
        logits = logits.clamp(-50.0, 50.0);

        logits
    }

    /// Save this model and its config to a checkpoint directory
    pub fn save_checkpoint(
        &self,
        config: &NanoChatConfig,
        checkpoint_dir: impl AsRef<Path>,
    ) -> anyhow::Result<()> {
        crate::checkpoint::save_checkpoint(self, config, checkpoint_dir)
    }

    /// Load a model from checkpoint directory
    pub fn load_checkpoint(
        checkpoint_dir: impl AsRef<Path>,
        device: &B::Device,
    ) -> anyhow::Result<(Self, NanoChatConfig)> {
        crate::checkpoint::load_checkpoint(checkpoint_dir, device)
    }
}
